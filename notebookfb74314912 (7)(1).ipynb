{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9",
      "identifier": "legacy",
      "language": "python",
      "language_version": "3.9",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "selected_hardware_size": "small",
    "kernel_info": {
      "name": "python3"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "e6751807-b41d-4e7f-b7a8-61af28d5acfc",
      "cell_type": "markdown",
      "source": "# Importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:37:59.736919Z",
          "iopub.execute_input": "2023-11-13T20:37:59.737321Z",
          "iopub.status.idle": "2023-11-13T20:38:01.167019Z",
          "shell.execute_reply.started": "2023-11-13T20:37:59.737243Z",
          "shell.execute_reply": "2023-11-13T20:38:01.166077Z"
        },
        "trusted": true
      },
      "execution_count": 1,
      "outputs": [],
      "id": "39d2b8b4"
    },
    {
      "cell_type": "code",
      "source": "# Load the datasets\ntrain_data_path = '../input/house-price-prediction-challenge/train.csv'\ntest_data_path = '../input/house-price-prediction-challenge/test.csv'\n\ntrain_data = pd.read_csv(train_data_path)\ntest_data = pd.read_csv(test_data_path)",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:01.168596Z",
          "iopub.execute_input": "2023-11-13T20:38:01.169012Z",
          "iopub.status.idle": "2023-11-13T20:38:01.433765Z",
          "shell.execute_reply.started": "2023-11-13T20:38:01.168984Z",
          "shell.execute_reply": "2023-11-13T20:38:01.432814Z"
        },
        "trusted": true
      },
      "execution_count": 2,
      "outputs": [],
      "id": "8755a203"
    },
    {
      "id": "51e38ff4-d97e-4025-be5c-e8d7d1c3a057",
      "cell_type": "markdown",
      "source": "# Loading the Datasets\nThis cell loads the training and test datasets from their respective CSV files using pandas.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "b4049f43-6f77-46f7-bc93-d01317ad7423",
      "cell_type": "markdown",
      "source": "# Displaying Basic Information about the Datasets\nThis cell displays basic information about the training and test datasets, such as the number of rows, columns, and the data types of each column.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "# Displaying basic information about the datasets\ntrain_info = train_data.info()\ntest_info = test_data.info()",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:01.435135Z",
          "iopub.execute_input": "2023-11-13T20:38:01.435467Z",
          "iopub.status.idle": "2023-11-13T20:38:01.488776Z",
          "shell.execute_reply.started": "2023-11-13T20:38:01.435441Z",
          "shell.execute_reply": "2023-11-13T20:38:01.487956Z"
        },
        "trusted": true,
        "noteable": {
          "output_collection_id": "edac1c81-e774-4501-bcaf-39d7e33e9cdd"
        }
      },
      "execution_count": 3,
      "outputs": [],
      "id": "8c30fc44"
    },
    {
      "cell_type": "code",
      "source": "# Displaying the first few rows of the training data for a quick overview\ntrain_head = train_data.head()\ntrain_head\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:01.490632Z",
          "iopub.execute_input": "2023-11-13T20:38:01.491435Z",
          "iopub.status.idle": "2023-11-13T20:38:01.511318Z",
          "shell.execute_reply.started": "2023-11-13T20:38:01.491383Z",
          "shell.execute_reply": "2023-11-13T20:38:01.510147Z"
        },
        "trusted": true,
        "noteable": {
          "output_collection_id": "f82fa882-95f4-419f-a6d9-6c926cdafb6f"
        }
      },
      "execution_count": 4,
      "outputs": [],
      "id": "5c0d2bc3"
    },
    {
      "id": "d110b504-0957-46d8-a1e9-555a13b1d373",
      "cell_type": "markdown",
      "source": "# Visualization Setup\nThis cell sets up the visual preferences for the seaborn library, which will be used for data visualization throughout the notebook.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "68ef6bdb-d9b3-4c76-8345-188ad681bb98",
      "cell_type": "markdown",
      "source": "# Preview of Training Data\nThis cell provides a quick preview of the first few rows of the training dataset, giving an initial sense of the data structure and values.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "# Setting up the visualisation preferences\nsns.set(style=\"whitegrid\")",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:01.512698Z",
          "iopub.execute_input": "2023-11-13T20:38:01.513614Z",
          "iopub.status.idle": "2023-11-13T20:38:01.518800Z",
          "shell.execute_reply.started": "2023-11-13T20:38:01.513582Z",
          "shell.execute_reply": "2023-11-13T20:38:01.517622Z"
        },
        "trusted": true
      },
      "execution_count": 5,
      "outputs": [],
      "id": "6c902c5d"
    },
    {
      "id": "528166c9-3b17-48d2-9855-af557c9bb233",
      "cell_type": "markdown",
      "source": "# Exploratory Data Analysis: Target Variable Distribution\nThis cell performs an exploratory data analysis (EDA) on the target variable, 'TARGET(PRICE_IN_LACS)', by plotting its distribution. The histogram helps in understanding the spread and skewness of house prices.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "# Exploratory Data Analysis (EDA)\n\n# 1. Target Variable Distribution\nplt.figure(figsize=(12, 6))\nsns.histplot(train_data['TARGET(PRICE_IN_LACS)'], kde=True, bins=50)\nplt.title('Distribution of House Prices')\nplt.xlabel('Price (in Lacs)')\nplt.ylabel('Frequency')\nplt.show()\n\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:01.520137Z",
          "iopub.execute_input": "2023-11-13T20:38:01.520446Z",
          "iopub.status.idle": "2023-11-13T20:38:02.230932Z",
          "shell.execute_reply.started": "2023-11-13T20:38:01.520421Z",
          "shell.execute_reply": "2023-11-13T20:38:02.229846Z"
        },
        "trusted": true,
        "noteable": {
          "output_collection_id": "e229b00c-ebfa-48e2-8e9e-f7307734230e"
        }
      },
      "execution_count": 6,
      "outputs": [],
      "id": "05325425"
    },
    {
      "id": "7e2f2e73-c6be-48fb-82ca-2a3d8421cc32",
      "cell_type": "markdown",
      "source": "# Exploratory Data Analysis: Numerical Features Distribution\nThis cell visualizes the distribution of key numerical features in the dataset, such as 'SQUARE_FT', 'LONGITUDE', and 'LATITUDE'. Histograms for these features provide insights into their range, central tendency, and spread.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "# 2. Numerical Features Distribution\nnumerical_features = ['SQUARE_FT', 'LONGITUDE', 'LATITUDE']\n\nplt.figure(figsize=(15, 5))\nfor i, feature in enumerate(numerical_features, 1):\n    plt.subplot(1, 3, i)\n    sns.histplot(train_data[feature], kde=True, bins=30)\n    plt.title(f'Distribution of {feature}')\n    plt.xlabel(feature)\n    plt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:02.232128Z",
          "iopub.execute_input": "2023-11-13T20:38:02.232472Z",
          "iopub.status.idle": "2023-11-13T20:38:04.190747Z",
          "shell.execute_reply.started": "2023-11-13T20:38:02.232443Z",
          "shell.execute_reply": "2023-11-13T20:38:04.189635Z"
        },
        "trusted": true,
        "noteable": {
          "output_collection_id": "cc6148ee-b9f1-46a5-afa5-eccbf54e87be"
        }
      },
      "execution_count": 7,
      "outputs": [],
      "id": "d7af0281"
    },
    {
      "id": "90a1afb8-a13e-4967-9340-0e719a87dc7b",
      "cell_type": "markdown",
      "source": "# Exploratory Data Analysis: Categorical Features Distribution\nThis cell focuses on the distribution of various categorical features in the dataset, including 'POSTED_BY', 'UNDER_CONSTRUCTION', 'RERA', and others. Count plots for these features help in understanding their frequency and distribution in the dataset.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "# 3. Categorical Features Distribution\ncategorical_features = ['POSTED_BY', 'UNDER_CONSTRUCTION', 'RERA', 'BHK_NO.', 'BHK_OR_RK', 'READY_TO_MOVE', 'RESALE']\n\nplt.figure(figsize=(15, 10))\nfor i, feature in enumerate(categorical_features, 1):\n    plt.subplot(3, 3, i)\n    sns.countplot(x=feature, data=train_data)\n    plt.title(f'Distribution of {feature}')\n    plt.xlabel(feature)\n    plt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:04.192553Z",
          "iopub.execute_input": "2023-11-13T20:38:04.193395Z",
          "iopub.status.idle": "2023-11-13T20:38:05.846168Z",
          "shell.execute_reply.started": "2023-11-13T20:38:04.193352Z",
          "shell.execute_reply": "2023-11-13T20:38:05.844936Z"
        },
        "trusted": true,
        "noteable": {
          "output_collection_id": "73d26c1a-8495-46c6-a669-a3171d50f6a2"
        }
      },
      "execution_count": 8,
      "outputs": [],
      "id": "b39159fb"
    },
    {
      "id": "c9c2cc35-41a0-44ae-bfde-a2c4ae8bbba1",
      "cell_type": "markdown",
      "source": "# Correlation Matrix Analysis\nThis cell creates a heatmap to visualize the correlation matrix of the numeric features in the dataset. The correlation matrix is crucial for identifying relationships between variables, which can inform feature selection and model building.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "# 4. Correlation Matrix\n# Selecting only numeric features for the correlation matrix\nnumeric_data = train_data.select_dtypes(include=['int64', 'float64'])\n\nplt.figure(figsize=(12, 8))\nsns.heatmap(numeric_data.corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:05.847569Z",
          "iopub.execute_input": "2023-11-13T20:38:05.848618Z",
          "iopub.status.idle": "2023-11-13T20:38:06.727123Z",
          "shell.execute_reply.started": "2023-11-13T20:38:05.848575Z",
          "shell.execute_reply": "2023-11-13T20:38:06.726005Z"
        },
        "trusted": true,
        "noteable": {
          "output_collection_id": "401d7f85-8a84-43a5-b9e2-7364454458f5"
        }
      },
      "execution_count": 9,
      "outputs": [],
      "id": "04bf725c"
    },
    {
      "id": "4e7476e9-6896-42c7-add9-03ffb550e8d6",
      "cell_type": "markdown",
      "source": "# Data Preprocessing: Handling Missing Values\nThis cell deals with the identification and handling of missing values in the dataset. Various strategies such as imputation or removal of rows/columns with missing values can be applied based on the nature and extent of the missing data.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "# Creating the Price Per Square Foot feature\n# train_data['PRICE_PER_SQFT'] = train_data['TARGET(PRICE_IN_LACS)'] * 100000 / train_data['SQUARE_FT']\n\n# Categorizing the 'SQUARE_FT' feature into size brackets\n# We will define small, medium, and large based on the distribution of 'SQUARE_FT'\nsquare_ft_quantiles = train_data['SQUARE_FT'].quantile([0.33, 0.66])\nsmall_threshold = square_ft_quantiles[0.33]\nmedium_threshold = square_ft_quantiles[0.66]\n\ndef categorize_size(sqft):\n    if sqft <= small_threshold:\n        return 'Small'\n    elif sqft <= medium_threshold:\n        return 'Medium'\n    else:\n        return 'Large'\n\ntrain_data['SIZE_CATEGORY'] = train_data['SQUARE_FT'].apply(categorize_size)\n\n# Combining 'BHK_NO.' and 'BHK_OR_RK' into a single categorical feature\n# train_data['ROOM_TYPE'] = train_data['BHK_NO.'].astype(str) + '_' + train_data['BHK_OR_RK']\n\n# Displaying the new features\n# train_data[['PRICE_PER_SQFT', 'SIZE_CATEGORY', 'ROOM_TYPE']].head()\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:06.733048Z",
          "iopub.execute_input": "2023-11-13T20:38:06.733532Z",
          "iopub.status.idle": "2023-11-13T20:38:06.754583Z",
          "shell.execute_reply.started": "2023-11-13T20:38:06.733492Z",
          "shell.execute_reply": "2023-11-13T20:38:06.753540Z"
        },
        "trusted": true
      },
      "execution_count": 10,
      "outputs": [],
      "id": "ffad6c6d"
    },
    {
      "cell_type": "code",
      "source": "# Excluding the 'ADDRESS' column and including the new features in the feature set\nfeatures = train_data.drop(['TARGET(PRICE_IN_LACS)', 'ADDRESS'], axis=1)\ntarget = train_data['TARGET(PRICE_IN_LACS)']",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:06.755828Z",
          "iopub.execute_input": "2023-11-13T20:38:06.756220Z",
          "iopub.status.idle": "2023-11-13T20:38:06.764408Z",
          "shell.execute_reply.started": "2023-11-13T20:38:06.756184Z",
          "shell.execute_reply": "2023-11-13T20:38:06.763115Z"
        },
        "trusted": true
      },
      "execution_count": 11,
      "outputs": [],
      "id": "893aa146"
    },
    {
      "cell_type": "code",
      "source": "# Identifying non-numeric (categorical) columns in the dataset\ncategorical_cols = features.select_dtypes(include=['object']).columns\n\n# Displaying the categorical columns\ncategorical_cols\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:06.766092Z",
          "iopub.execute_input": "2023-11-13T20:38:06.766689Z",
          "iopub.status.idle": "2023-11-13T20:38:06.780624Z",
          "shell.execute_reply.started": "2023-11-13T20:38:06.766622Z",
          "shell.execute_reply": "2023-11-13T20:38:06.779419Z"
        },
        "trusted": true,
        "noteable": {
          "output_collection_id": "e6224559-ba8a-4977-a449-23cc5eb0bf81"
        }
      },
      "execution_count": 12,
      "outputs": [],
      "id": "ba0e496f"
    },
    {
      "cell_type": "code",
      "source": "# from sklearn.model_selection import GridSearchCV\n\n# # Defining the parameter grid for hyperparameter tuning\n# param_grid = {\n#     'n_estimators': [100, 200, 300],\n#     'learning_rate': [0.01, 0.1, 0.2],\n#     'max_depth': [3, 4, 5],\n#     'min_samples_split': [2, 4, 6],\n#     'min_samples_leaf': [1, 2, 3]\n# }\n\n# # Initializing the Grid Search with the Gradient Boosting Regressor and the parameter grid\n# grid_search = GridSearchCV(estimator=GradientBoostingRegressor(random_state=42), \n#                            param_grid=param_grid, \n#                            cv=3, \n#                            n_jobs=-1, \n#                            verbose=2)\n\n# # Fitting the grid search to the data\n# grid_search.fit(X_train, y_train)\n\n# # Getting the best parameters\n# best_params = grid_search.best_params_\n\n# best_params\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:06.782290Z",
          "iopub.execute_input": "2023-11-13T20:38:06.782701Z",
          "iopub.status.idle": "2023-11-13T20:38:06.787707Z",
          "shell.execute_reply.started": "2023-11-13T20:38:06.782662Z",
          "shell.execute_reply": "2023-11-13T20:38:06.786644Z"
        },
        "trusted": true
      },
      "execution_count": 13,
      "outputs": [],
      "id": "e9a8efd2"
    },
    {
      "cell_type": "code",
      "source": "# Extracting city and neighborhood from the 'ADDRESS' field\ntrain_data['NEIGHBORHOOD'] = train_data['ADDRESS'].apply(lambda x: x.split(',')[0].strip() if ',' in x else 'Unknown')\ntrain_data['CITY'] = train_data['ADDRESS'].apply(lambda x: x.split(',')[-1].strip() if ',' in x else 'Unknown')\n\n# Checking for any inconsistencies or unusual patterns in the extracted data\nneighborhood_counts = train_data['NEIGHBORHOOD'].value_counts()\ncity_counts = train_data['CITY'].value_counts()\n\nneighborhood_counts.head(10), city_counts.head(10)  # Displaying top 10 counts for each for a quick overview\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:06.788953Z",
          "iopub.execute_input": "2023-11-13T20:38:06.789424Z",
          "iopub.status.idle": "2023-11-13T20:38:06.852567Z",
          "shell.execute_reply.started": "2023-11-13T20:38:06.789389Z",
          "shell.execute_reply": "2023-11-13T20:38:06.851448Z"
        },
        "trusted": true,
        "noteable": {
          "output_collection_id": "aae73a51-a6ee-459e-a4a3-a2f510742ca7"
        }
      },
      "execution_count": 14,
      "outputs": [],
      "id": "40ab2f43"
    },
    {
      "cell_type": "code",
      "source": "# Proceeding with data cleaning and standardization for the 'CITY' and 'NEIGHBORHOOD' columns\n# This process includes addressing inconsistencies and standardizing the format\n\n# Handling known inconsistencies like 'Maharashtra' being listed as a city\n# We can categorize such entries as 'Other' or 'Unknown'\ntrain_data['CITY'] = train_data['CITY'].replace({'Maharashtra': 'Other'})\n\n# Standardizing the format: Removing leading and trailing spaces, if any\ntrain_data['CITY'] = train_data['CITY'].str.strip()\ntrain_data['NEIGHBORHOOD'] = train_data['NEIGHBORHOOD'].str.strip()\n\n# Checking for any obvious inconsistencies or unusual patterns after cleaning\ncleaned_city_counts = train_data['CITY'].value_counts()\ncleaned_neighborhood_counts = train_data['NEIGHBORHOOD'].value_counts()\n\ncleaned_city_counts.head(10), cleaned_neighborhood_counts.head(10)  # Displaying top 10 counts for each for a quick overview\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:06.854102Z",
          "iopub.execute_input": "2023-11-13T20:38:06.854765Z",
          "iopub.status.idle": "2023-11-13T20:38:06.902797Z",
          "shell.execute_reply.started": "2023-11-13T20:38:06.854724Z",
          "shell.execute_reply": "2023-11-13T20:38:06.901527Z"
        },
        "trusted": true,
        "noteable": {
          "output_collection_id": "64572c15-9ca0-4f1a-83de-3c20c3e2960e"
        }
      },
      "execution_count": 15,
      "outputs": [],
      "id": "bca0aba6"
    },
    {
      "cell_type": "code",
      "source": "# Conducting an Exploratory Data Analysis (EDA) for the new location-based features: 'CITY' and 'NEIGHBORHOOD'\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Setting up visualization style\nsns.set(style=\"whitegrid\")\n\n# Analyzing the distribution of house prices across different cities\nplt.figure(figsize=(15, 6))\nsns.boxplot(x='CITY', y='TARGET(PRICE_IN_LACS)', data=train_data)\nplt.xticks(rotation=45)\nplt.title('Distribution of House Prices Across Different Cities')\nplt.xlabel('City')\nplt.ylabel('Price (in Lacs)')\nplt.show()\n\n# Given the likely high cardinality of neighborhoods, we'll focus on the top 10 neighborhoods for a clearer view\ntop_neighborhoods = train_data['NEIGHBORHOOD'].value_counts().head(10).index\nfiltered_data = train_data[train_data['NEIGHBORHOOD'].isin(top_neighborhoods)]\n\nplt.figure(figsize=(15, 6))\nsns.boxplot(x='NEIGHBORHOOD', y='TARGET(PRICE_IN_LACS)', data=filtered_data)\nplt.xticks(rotation=45)\nplt.title('Distribution of House Prices in Top 10 Neighborhoods')\nplt.xlabel('Neighborhood')\nplt.ylabel('Price (in Lacs)')\nplt.show()\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:06.905428Z",
          "iopub.execute_input": "2023-11-13T20:38:06.906091Z",
          "iopub.status.idle": "2023-11-13T20:38:12.745695Z",
          "shell.execute_reply.started": "2023-11-13T20:38:06.906048Z",
          "shell.execute_reply": "2023-11-13T20:38:12.744624Z"
        },
        "trusted": true,
        "noteable": {
          "output_collection_id": "64f6cdc2-54be-4073-8a10-87167a83c520"
        }
      },
      "execution_count": 16,
      "outputs": [],
      "id": "a101b387"
    },
    {
      "cell_type": "code",
      "source": "# Extracting city and neighborhood from the 'ADDRESS' field\ntest_data['NEIGHBORHOOD'] = test_data['ADDRESS'].apply(lambda x: x.split(',')[0].strip() if ',' in x else 'Unknown')\ntest_data['CITY'] = test_data['ADDRESS'].apply(lambda x: x.split(',')[-1].strip() if ',' in x else 'Unknown')\n\n# Checking for any inconsistencies or unusual patterns in the extracted data\nneighborhood_counts = test_data['NEIGHBORHOOD'].value_counts()\ncity_counts = test_data['CITY'].value_counts()\n\nneighborhood_counts.head(10), city_counts.head(10)  # Displaying top 10 counts for each for a quick overview\n\n\n# Proceeding with data cleaning and standardization for the 'CITY' and 'NEIGHBORHOOD' columns\n# This process includes addressing inconsistencies and standardizing the format\n\n# Handling known inconsistencies like 'Maharashtra' being listed as a city\n# We can categorize such entries as 'Other' or 'Unknown'\ntest_data['CITY'] = test_data['CITY'].replace({'Maharashtra': 'Other'})\n\n# Standardizing the format: Removing leading and trailing spaces, if any\ntest_data['CITY'] = test_data['CITY'].str.strip()\ntest_data['NEIGHBORHOOD'] = test_data['NEIGHBORHOOD'].str.strip()\n\n# Checking for any obvious inconsistencies or unusual patterns after cleaning\ncleaned_city_counts = test_data['CITY'].value_counts()\ncleaned_neighborhood_counts = test_data['NEIGHBORHOOD'].value_counts()\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:12.747208Z",
          "iopub.execute_input": "2023-11-13T20:38:12.747669Z",
          "iopub.status.idle": "2023-11-13T20:38:12.925161Z",
          "shell.execute_reply.started": "2023-11-13T20:38:12.747628Z",
          "shell.execute_reply": "2023-11-13T20:38:12.924360Z"
        },
        "trusted": true
      },
      "execution_count": 17,
      "outputs": [],
      "id": "39ab1a40"
    },
    {
      "cell_type": "code",
      "source": "from category_encoders import TargetEncoder\n\n# Assuming train_data is already loaded\ncity_encoder = TargetEncoder()\nneighborhood_encoder = TargetEncoder()\n\n# Basic Target Encoding\ntrain_data['CITY_ENCODED'] = city_encoder.fit_transform(train_data['CITY'], train_data['TARGET(PRICE_IN_LACS)'])\ntrain_data['NEIGHBORHOOD_ENCODED'] = neighborhood_encoder.fit_transform(train_data['NEIGHBORHOOD'], train_data['TARGET(PRICE_IN_LACS)'])\n\n# Check the encoded columns\nprint(train_data[['CITY_ENCODED', 'NEIGHBORHOOD_ENCODED']].head())\n\n# Apply the same encoding to the test data\n# Assuming test_data is already loaded\ntest_data['CITY_ENCODED'] = city_encoder.transform(test_data['CITY'])\ntest_data['NEIGHBORHOOD_ENCODED'] = neighborhood_encoder.transform(test_data['NEIGHBORHOOD'])\n\n# Check the encoded columns in test data\nprint(test_data[['CITY_ENCODED', 'NEIGHBORHOOD_ENCODED']].head())\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:12.926366Z",
          "iopub.execute_input": "2023-11-13T20:38:12.927349Z",
          "iopub.status.idle": "2023-11-13T20:38:13.471708Z",
          "shell.execute_reply.started": "2023-11-13T20:38:12.927314Z",
          "shell.execute_reply": "2023-11-13T20:38:13.470642Z"
        },
        "trusted": true,
        "noteable": {
          "output_collection_id": "db054579-a78c-4c2a-9854-c91a961d13fc"
        }
      },
      "execution_count": 18,
      "outputs": [],
      "id": "5ff4e392"
    },
    {
      "cell_type": "code",
      "source": "# Integrating the encoded features 'CITY_ENCODED' and 'NEIGHBORHOOD_ENCODED' into the model\n# We'll update the feature set and prepare the data for model training\n\n# Dropping the original 'CITY' and 'NEIGHBORHOOD' columns\nfeatures = train_data.drop(['CITY', 'NEIGHBORHOOD', 'ADDRESS', 'TARGET(PRICE_IN_LACS)'], axis=1)\n\n# Including the encoded city and neighborhood features\nfeatures['CITY_ENCODED'] = train_data['CITY_ENCODED']\nfeatures['NEIGHBORHOOD_ENCODED'] = train_data['NEIGHBORHOOD_ENCODED']\n\n# Separating the target variable\ntarget = train_data['TARGET(PRICE_IN_LACS)']\n\n# Splitting the data into training and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(features, target, test_size=0.2, random_state=42)\n\n# Verifying the shapes of the resulting data splits\n(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:13.473143Z",
          "iopub.execute_input": "2023-11-13T20:38:13.473488Z",
          "iopub.status.idle": "2023-11-13T20:38:13.495881Z",
          "shell.execute_reply.started": "2023-11-13T20:38:13.473459Z",
          "shell.execute_reply": "2023-11-13T20:38:13.494841Z"
        },
        "trusted": true,
        "noteable": {
          "output_collection_id": "95b65f9c-40bb-4ce5-b697-e527bc9ffe36"
        }
      },
      "execution_count": 19,
      "outputs": [],
      "id": "51a411e6"
    },
    {
      "cell_type": "code",
      "source": "# Performing one-hot encoding on the categorical columns 'POSTED_BY' and 'BHK_OR_RK'\none_hot_encoded_data = pd.get_dummies(X_train, columns=categorical_cols)\n\n# Also applying the same encoding to the validation set to maintain consistency\none_hot_encoded_valid_data = pd.get_dummies(X_valid, columns=categorical_cols)\n\n# Ensuring that the columns in the validation set match those in the training set\none_hot_encoded_valid_data = one_hot_encoded_valid_data.reindex(columns=one_hot_encoded_data.columns, fill_value=0)\n\n# Verifying the transformation\none_hot_encoded_data.head(), one_hot_encoded_valid_data.head()\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:13.497204Z",
          "iopub.execute_input": "2023-11-13T20:38:13.497556Z",
          "iopub.status.idle": "2023-11-13T20:38:13.539750Z",
          "shell.execute_reply.started": "2023-11-13T20:38:13.497528Z",
          "shell.execute_reply": "2023-11-13T20:38:13.538758Z"
        },
        "trusted": true,
        "noteable": {
          "output_collection_id": "d8f03d0c-7cc1-489a-8105-5ad342d6c717"
        }
      },
      "execution_count": 20,
      "outputs": [],
      "id": "7942a5e2"
    },
    {
      "cell_type": "code",
      "source": "# Identifying outliers in the 'SQUARE_FT' feature using the Interquartile Range (IQR) method\n\n# Calculating IQR\nQ1 = train_data['SQUARE_FT'].quantile(0.25)\nQ3 = train_data['SQUARE_FT'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Defining thresholds for outliers\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n\n# Identifying outliers\noutliers = train_data[(train_data['SQUARE_FT'] < lower_bound) | (train_data['SQUARE_FT'] > upper_bound)]\n\n# Percentage of data that are outliers\noutlier_percentage = len(outliers) / len(train_data) * 100\n\n# Displaying the results\nlower_bound, upper_bound, outlier_percentage, outliers.head()\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:13.541028Z",
          "iopub.execute_input": "2023-11-13T20:38:13.541352Z",
          "iopub.status.idle": "2023-11-13T20:38:13.561191Z",
          "shell.execute_reply.started": "2023-11-13T20:38:13.541325Z",
          "shell.execute_reply": "2023-11-13T20:38:13.560498Z"
        },
        "trusted": true,
        "noteable": {
          "output_collection_id": "cb3bb4eb-ff4f-4034-86f1-66249976f676"
        }
      },
      "execution_count": 21,
      "outputs": [],
      "id": "1ed30a5a"
    },
    {
      "cell_type": "code",
      "source": "# Applying a logarithmic transformation to the 'SQUARE_FT' feature to reduce the impact of outliers\n\n# Adding a small constant to avoid issues with log(0)\nconstant = 1e-3\ntrain_data['SQUARE_FT_LOG'] = np.log(train_data['SQUARE_FT'] + constant)\n\n# Applying the same transformation to the training and validation feature sets\none_hot_encoded_data['SQUARE_FT_LOG'] = np.log(one_hot_encoded_data['SQUARE_FT'] + constant)\none_hot_encoded_valid_data['SQUARE_FT_LOG'] = np.log(one_hot_encoded_valid_data['SQUARE_FT'] + constant)\n\n# Removing the original 'SQUARE_FT' feature to avoid redundancy\none_hot_encoded_data = one_hot_encoded_data.drop('SQUARE_FT', axis=1)\none_hot_encoded_valid_data = one_hot_encoded_valid_data.drop('SQUARE_FT', axis=1)\n\n# Displaying the first few rows of the transformed data\none_hot_encoded_data[['SQUARE_FT_LOG']].head(), one_hot_encoded_valid_data[['SQUARE_FT_LOG']].head()\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:13.562312Z",
          "iopub.execute_input": "2023-11-13T20:38:13.562788Z",
          "iopub.status.idle": "2023-11-13T20:38:13.581878Z",
          "shell.execute_reply.started": "2023-11-13T20:38:13.562760Z",
          "shell.execute_reply": "2023-11-13T20:38:13.580955Z"
        },
        "trusted": true,
        "noteable": {
          "output_collection_id": "77972f0c-d3d5-42f6-8866-c51165c07630"
        }
      },
      "execution_count": 22,
      "outputs": [],
      "id": "fb890276"
    },
    {
      "cell_type": "code",
      "source": "# Post-processing to handle negative predictions\nmin_price = train_data['TARGET(PRICE_IN_LACS)'].min()\nprint(\"Minimum price in training set:\", min_price)\n\ndef handle_negative_predictions(predictions):\n    return np.maximum(predictions, min_price)\n\n# Example usage\n# predictions = model.predict(X_test)\n# corrected_predictions = handle_negative_predictions(predictions)\n# Post-processing step to handle negative predictions\ndef handle_negative_predictions(predictions):\n    min_price = train_data['TARGET(PRICE_IN_LACS)'].min()\n    return np.maximum(predictions, min_price)\n\n# Example usage\n# predictions = model.predict(X_test)\n# corrected_predictions = handle_negative_predictions(predictions)",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:13.583254Z",
          "iopub.execute_input": "2023-11-13T20:38:13.583562Z",
          "iopub.status.idle": "2023-11-13T20:38:13.590081Z",
          "shell.execute_reply.started": "2023-11-13T20:38:13.583537Z",
          "shell.execute_reply": "2023-11-13T20:38:13.588983Z"
        },
        "trusted": true,
        "noteable": {
          "output_collection_id": "6b26e1fd-77dc-49fe-9431-5e379836b430"
        }
      },
      "execution_count": 23,
      "outputs": [],
      "id": "eb34df17"
    },
    {
      "id": "c24c9cbf-b61a-427d-a16e-9858eb836dde",
      "cell_type": "markdown",
      "source": "# Model Training: Gradient Boosting Regressor\nThis cell involves the training of a Gradient Boosting Regressor model. The model is trained on the preprocessed training data, and its performance is evaluated on the validation set.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "id": "61d3ed13-366e-4904-bfd0-c44a4e54e65e",
      "cell_type": "markdown",
      "source": "# Model Evaluation: MSE and R2 Score\nThis cell evaluates the performance of the trained Gradient Boosting Regressor model using metrics such as Mean Squared Error (MSE) and R2 Score. These metrics provide insights into the accuracy and predictive power of the model.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "# Retraining the Gradient Boosting Regressor with the optimized hyperparameters\n\n# Setting the optimized parameters\noptimal_gb_model = GradientBoostingRegressor(learning_rate=0.2,\n    max_depth=3,\n    min_samples_leaf=1,\n    min_samples_split=2,\n    n_estimators=300,\n    random_state=42)\n\n# Training the model on the training dataset\noptimal_gb_model.fit(one_hot_encoded_data, y_train)\n\n# Predicting on the validation set\noptimal_y_pred = optimal_gb_model.predict(one_hot_encoded_valid_data)\noptimal_y_pred = handle_negative_predictions(optimal_y_pred)\n\n# Calculating the performance metrics\noptimal_mse = mean_squared_error(y_valid, optimal_y_pred)\noptimal_r2 = r2_score(y_valid, optimal_y_pred)\n\n(optimal_mse, optimal_r2)\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:38:47.051950Z",
          "iopub.execute_input": "2023-11-13T20:38:47.052354Z",
          "iopub.status.idle": "2023-11-13T20:38:57.938680Z",
          "shell.execute_reply.started": "2023-11-13T20:38:47.052323Z",
          "shell.execute_reply": "2023-11-13T20:38:57.937447Z"
        },
        "trusted": true,
        "noteable": {
          "output_collection_id": "86d39cda-3c9a-488e-85a1-88d6635cca83"
        }
      },
      "execution_count": 25,
      "outputs": [],
      "id": "fb7c04e4"
    },
    {
      "cell_type": "code",
      "source": "# Creating the Price Per Square Foot feature\n# test_data['PRICE_PER_SQFT'] = test_data['TARGET(PRICE_IN_LACS)'] * 100000 / test_data['SQUARE_FT']\n\n# Categorizing the 'SQUARE_FT' feature into size brackets\n# We will define small, medium, and large based on the distribution of 'SQUARE_FT'\nsquare_ft_quantiles = test_data['SQUARE_FT'].quantile([0.33, 0.66])\nsmall_threshold = square_ft_quantiles[0.33]\nmedium_threshold = square_ft_quantiles[0.66]\n\ndef categorize_size(sqft):\n    if sqft <= small_threshold:\n        return 'Small'\n    elif sqft <= medium_threshold:\n        return 'Medium'\n    else:\n        return 'Large'\n\ntest_data['SIZE_CATEGORY'] = test_data['SQUARE_FT'].apply(categorize_size)\n\n# Combining 'BHK_NO.' and 'BHK_OR_RK' into a single categorical feature\n# test_data['ROOM_TYPE'] = test_data['BHK_NO.'].astype(str) + '_' + test_data['BHK_OR_RK']\n\n# Displaying the new features\n# test_data[[ 'SIZE_CATEGORY', 'ROOM_TYPE']].head()\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:39:10.399221Z",
          "iopub.execute_input": "2023-11-13T20:39:10.399610Z",
          "iopub.status.idle": "2023-11-13T20:39:10.432702Z",
          "shell.execute_reply.started": "2023-11-13T20:39:10.399582Z",
          "shell.execute_reply": "2023-11-13T20:39:10.431835Z"
        },
        "trusted": true
      },
      "execution_count": 26,
      "outputs": [],
      "id": "b15957f1"
    },
    {
      "cell_type": "code",
      "source": "features_test = test_data.drop(['CITY', 'NEIGHBORHOOD', 'ADDRESS'], axis=1)\n# Identifying non-numeric (categorical) columns in the dataset\ncategorical_cols1 = features_test.select_dtypes(include=['object']).columns\n\n# Displaying the categorical columns\ncategorical_cols1\n",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:39:13.482005Z",
          "iopub.execute_input": "2023-11-13T20:39:13.482393Z",
          "iopub.status.idle": "2023-11-13T20:39:13.509037Z",
          "shell.execute_reply.started": "2023-11-13T20:39:13.482362Z",
          "shell.execute_reply": "2023-11-13T20:39:13.508216Z"
        },
        "trusted": true,
        "noteable": {
          "output_collection_id": "a277fd08-0b49-4c25-a42c-88a65935265d"
        }
      },
      "execution_count": 27,
      "outputs": [],
      "id": "01666e22"
    },
    {
      "cell_type": "code",
      "source": "one_hot_encoded_data_test = pd.get_dummies(features_test, columns=categorical_cols1)",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:39:15.827594Z",
          "iopub.execute_input": "2023-11-13T20:39:15.828102Z",
          "iopub.status.idle": "2023-11-13T20:39:15.883825Z",
          "shell.execute_reply.started": "2023-11-13T20:39:15.828060Z",
          "shell.execute_reply": "2023-11-13T20:39:15.882618Z"
        },
        "trusted": true
      },
      "execution_count": 28,
      "outputs": [],
      "id": "097dc402"
    },
    {
      "cell_type": "code",
      "source": "# Adding a small constant to avoid issues with log(0)\nconstant = 1e-3\none_hot_encoded_data_test['SQUARE_FT_LOG'] = np.log(one_hot_encoded_data_test['SQUARE_FT'] + constant)\none_hot_encoded_data_test = one_hot_encoded_data_test.drop('SQUARE_FT', axis=1)",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:39:18.792372Z",
          "iopub.execute_input": "2023-11-13T20:39:18.793257Z",
          "iopub.status.idle": "2023-11-13T20:39:18.804230Z",
          "shell.execute_reply.started": "2023-11-13T20:39:18.793202Z",
          "shell.execute_reply": "2023-11-13T20:39:18.803041Z"
        },
        "trusted": true
      },
      "execution_count": 29,
      "outputs": [],
      "id": "df1ea265"
    },
    {
      "id": "1a8e4561-ef9c-45a5-a800-aac005642742",
      "cell_type": "markdown",
      "source": "# Saving Predictions to CSV\nThis cell demonstrates how to save the model's predictions into a CSV file. This is useful for further analysis or submission in case of a machine learning competition.",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        }
      }
    },
    {
      "cell_type": "code",
      "source": "optimal_y_pred_test = optimal_gb_model.predict(one_hot_encoded_data_test)\n\noptimal_y_pred_df = pd.DataFrame(optimal_y_pred, columns=['Predicted_Price'])\noptimal_y_pred_df.to_csv('optimal_y_pred.csv', index=False)\nprint('Predictions saved to optimal_y_pred.csv')",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-11-13T20:39:20.667688Z",
          "iopub.execute_input": "2023-11-13T20:39:20.668099Z",
          "iopub.status.idle": "2023-11-13T20:39:20.905896Z",
          "shell.execute_reply.started": "2023-11-13T20:39:20.668068Z",
          "shell.execute_reply": "2023-11-13T20:39:20.904869Z"
        },
        "trusted": true,
        "noteable": {
          "output_collection_id": "e664e632-8a87-4bae-9166-3ec1cae2fda8"
        }
      },
      "execution_count": 30,
      "outputs": [],
      "id": "c0b24b90"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "id": "797579d5"
    }
  ]
}